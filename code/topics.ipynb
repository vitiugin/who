{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package wordnet to /Users/fv/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /Users/fv/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "# Based on code from https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from spacy.lang.en import English\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "spacy.load('en_core_web_sm')\n",
    "#spacy.load('es_core_news_sm')\n",
    "#spacy.load(\"fr_core_news_sm\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "#en_stop = set(nltk.corpus.stopwords.words('spanish'))\n",
    "#en_stop = set(nltk.corpus.stopwords.words('french'))\n",
    "\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topics(path, num_topics):\n",
    "    text_data = []\n",
    "    f = pd.read_csv(path, 'rb', delimiter = '\\t')\n",
    "    for line in f['text']:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .99:\n",
    "            #print('List of tokens:', tokens)\n",
    "            text_data.append(tokens)\n",
    "\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "    pickle.dump(corpus, open('models/corpus.pkl', 'wb'))\n",
    "    dictionary.save('models/dictionary.gensim')\n",
    "\n",
    "    NUM_TOPICS = num_topics\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "    ldamodel.save('models/model' + str(NUM_TOPICS) + '.gensim')\n",
    "    topics = ldamodel.print_topics(num_words=10)\n",
    "    for topic in topics:\n",
    "        print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(0, '0.063*\"covid19\" + 0.063*\"coronavirus\" + 0.034*\"whorajray\" + 0.034*\"nitaambani\" + 0.034*\"mukeshambani\" + 0.034*\"narendramodi\" + 0.034*\"indiafightscorona\" + 0.034*\"rajray\" + 0.034*\"111/138\" + 0.034*\"everybreathcounts\"')\n(1, '0.048*\"covid19\" + 0.048*\"trump\" + 0.048*\"propangandized\" + 0.048*\"virus\" + 0.048*\"death\" + 0.048*\"distance\" + 0.048*\"canada\" + 0.048*\"stewardship\" + 0.048*\"dither\" + 0.048*\"deadly\"')\n(2, '0.043*\"disponible\" + 0.043*\"pandemia\" + 0.043*\"combatir\" + 0.043*\"dise√±ada\" + 0.043*\"lanzamiento\" + 0.043*\"whoacademy\" + 0.043*\"vidas\" + 0.043*\"ampliar\" + 0.043*\"trabajadores\" + 0.043*\"permitir\"')\n(3, '0.048*\"tecikorodu\" + 0.048*\"precaution\" + 0.048*\"healing\" + 0.048*\"adhere\" + 0.048*\"people\" + 0.048*\"trust\" + 0.048*\"recover\" + 0.048*\"encourage\" + 0.048*\"safety\" + 0.048*\"beyondordinary\"')\n(4, '0.017*\"covid19\" + 0.017*\"indiafightscorona\" + 0.017*\"rajray\" + 0.017*\"mukeshambani\" + 0.017*\"whorajray\" + 0.017*\"nitaambani\" + 0.017*\"narendramodi\" + 0.017*\"coronavirus\" + 0.017*\"confirm\" + 0.017*\"income\"')\n"
    }
   ],
   "source": [
    "get_topics('data/out/IHorgs/a14_who/a14_who.csv', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "who",
   "display_name": "who"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}